{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer, OneHotEncoder\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the data\ntrain = pd.read_csv(\"../input/train.csv\")\nX_train_full = np.asarray(train.iloc[:, 1:], dtype=np.float32)\ny_train_full = np.asarray(train.iloc[:, 0], dtype=np.float32).reshape(train.shape[0], 1)\ntest = pd.read_csv(\"../input/test.csv\")\nX_test = np.asarray(test, dtype=np.float32)\n\n# Normalize to [0, 1] for the NN\nnormalizer = Normalizer(norm = \"l2\", copy=False)\nX_train_full = normalizer.transform(X_train_full)\nX_test = normalizer.transform(X_test)\n\n# Put in 2d shape\nX_train_full = X_train_full.reshape((train.shape[0], 28, 28))\nX_test = X_test.reshape((test.shape[0], 28, 28))\n\n# One-hot encoding of the output\n# one_hot_enc = OneHotEncoder(categories=\"auto\", dtype=np.int)\none_hot_enc = OneHotEncoder(n_values=\"auto\", dtype=np.int)\ny_train_number = y_train_full.flatten().astype(np.int64) # for plotting\ny_train_full = one_hot_enc.fit_transform(y_train_full)\nprint(y_train_full.shape)\n\n# Define a validation set for the training \nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_number, test_size=0.25)\nprint(X_train.shape, X_valid.shape, X_test.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24ee58b970269ab4a3bb3233a6cefd63a25c8f4b",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Visualize the data\nfor digit in range(10):\n    input_bool_indices = y_train_number == digit\n    avg_digit = np.mean(X_train_full[input_bool_indices, :, :], axis=0)\n    med_digit = np.median(X_train_full[input_bool_indices, :, :], axis=0)\n    fig = plt.figure(digit)\n    fig.add_subplot(1, 2, 1)\n    plt.imshow(avg_digit, cmap=cm.Greys)\n    fig.add_subplot(1, 2, 2)\n    plt.imshow(med_digit, cmap=cm.Greys)\n# fig.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3489127b8a2bbb5247559904fb35da4dc1e59c77",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Prepare inputs for PyTorch\nclass MnistDataset(Dataset):\n    def __init__(self, X, y, transforms=[]):\n        self.X = X\n        self.y = y#.toarray()\n        self.transforms = transforms\n    \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, index):\n        X = torch.from_numpy(self.X[index, :, :].reshape((1, 28, 28)))\n        for trans in self.transforms:\n            X = trans(X)\n        return X, torch.from_numpy(np.array(self.y[index]))\n\n\ndata_params = {'batch_size': 128,\n              'shuffle': True,\n              'num_workers': 4}\n   \ntransforms_list = []\ntrain_ds = MnistDataset(X_train, y_train_number, transforms_list)\nvalid_ds = MnistDataset(X_valid, y_valid, transforms_list)\ntrain_loader = DataLoader(train_ds, **data_params)\nvalid_loader = DataLoader(valid_ds, **data_params)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a95be41683de610368b371ea461bc2d63575209",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class MnistNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n        self.fc1 = nn.Linear(512, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        # (28 - 5 + 1) ** 2 * 8 = 24 ** 2 * 8\n        x = self.conv1(x)\n        # ((24 - 2) / 2 + 1) ** 2 * 8   12 ** 2 * 8\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n        # (12 - 5 + 1) ** 2 * 16 = 8 ** 2 * 16\n        x = self.conv2(x)\n        x = F.dropout2d(x)\n        #  ((8 - 2) / 2 + 1) ** 2 * 16 = 4 ** 2 * 16 = 256\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n        # Flatten layer\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.softmax(x, dim=0)\n\nmodel = MnistNN()\n# model = MnistNN().cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95425e12ee1a46f1a986691f3842a6a4c5e5bc56",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def train(epoch):\n    model.train()\n    for train_batch_indx, (Xt, yt) in enumerate(train_loader):\n        optimizer.zero_grad()\n        Xt, yt = Variable(Xt), Variable(yt)\n        # Xt, yt = Xt.cuda(async=True), yt.cuda(async=True) # On GPU\n        train_pred = model(Xt)\n        train_loss = F.cross_entropy(train_pred, yt)           \n        # Start backwards step of backprop\n        train_loss.backward()\n        optimizer.step()\n        if train_batch_indx % 50 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTLoss: {:.6f}'.format(\n                epoch, \n                train_batch_indx * len(Xt), len(train_loader.dataset),\n                100. * train_batch_indx / len(train_loader), \n                train_loss.item()))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f249ff64f224beee44fdeda010c8cf3b1d759c1"
      },
      "cell_type": "code",
      "source": "# torch.cuda.get_device_name(0)\n\nfrom sklearn.metrics import confusion_matrix\nfor epoch in range(10):\n    conf_mat = np.ndarray((10, 10))\n    train(epoch)\n    for Xv, yv in valid_loader:\n        Xv, yv = Variable(Xv), Variable(yv)\n        # Xv, yv = Xv.cuda(async=True), yv.cuda(async=True) # On GPU\n        valid_prob = model(Xv)\n        # print(valid_prob)\n        valid_pred = valid_prob.argmax(dim=1)\n        # print(valid_pred.shape)\n        conf_mat += confusion_matrix(yv, valid_pred, labels=list(range(10)))\n    print(conf_mat)\n    print(\"--\")\n\nprint(\"---------------------\")\nprint(conf_mat)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}