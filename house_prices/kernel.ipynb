{
  "cells": [
    {
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": false,
        "_cell_guid": "00e7a874-1d17-49ce-8950-0fd3f3838b44",
        "_uuid": "c0f70245e3a7c9e42aa0dcb52156c8599232df75",
        "_kg_hide-input": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO:\n# See whether some outliers should be removed\n# See whether using a box-cox transform on non-normal numeric variables would help\n\nimport os.path\nimport time\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.linear_model import (ElasticNet, ElasticNetCV, Lasso, LassoCV, \n                                  LinearRegression, Perceptron)\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import WhiteKernel, RationalQuadratic, RBF\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import RepeatedKFold, KFold, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MaxAbsScaler\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import cross_val_predict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nRNG_SEED = int(time.time())\nprint(\"Seed: %s\" % RNG_SEED)\n\noverwrite_models = True\nadd_was_missing_features = False\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# os.listdir(\"..\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "_kg_hide-output": false,
        "_cell_guid": "7d3f4025-b913-4f38-a51f-8034e422f51d",
        "_uuid": "77108608f9d8328fc6b86d6b99da28a390a0730d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\")\n# Use the log price as it is the target for the evaluation. \n# Moreover, this suppresses the skewness.\noutcome_df = np.log1p(train_df.loc[:, \"SalePrice\"])\ntrain_ids = train_df[\"Id\"]\nfeature_df = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n\ntest_df = pd.read_csv(\"../input/test.csv\")\ntest_ids = test_df[\"Id\"]\ntest_df = test_df.drop(\"Id\", axis=1)\n\nprint(\"train set size: %s x %s\" % feature_df.shape)\nprint(\"outcome size: %s\" % outcome_df.shape)\nprint(\"test set size: %s x %s\" % test_df.shape)\n\n# Quick check for the outcome variable\nplt.figure()\nax = sns.distplot(outcome_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "_cell_guid": "8d335a53-61c1-47db-bac8-e6b8edbf7c20",
        "_uuid": "b3cba709312ebf2b59d89ea09cb5deac736cdd66",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Handling missing data\n# We merge the train and test set to compute the modes and medians \n# since the calculations do not rely on the outcome.\ndef get_replacement(data):\n# def get_replacement(train_set, test_set):\n    # data = pd.concat([train_set, test_set])\n    nb_nan_per_col = data.shape[0] - data.count()\n    print(nb_nan_per_col[nb_nan_per_col != 0])\n    \n    missing_val_replace = {}\n    \n    # Type of zone (residential, commercial etc.), cannot be guessed \n    # with current data. Set the mode.\n    missing_val_replace[\"MSZoning\"] = data[\"MSZoning\"].mode()[0]\n    \n    # either 'AllPub' or 'NoSeWa'. Set the mode.\n    missing_val_replace[\"Utilities\"] = data[\"Utilities\"].mode()[0]\n        \n    test_df.loc[np.any(pd.isnull(test_df), axis=1), nb_nan_per_col != 0]\n    \n    # Linear feet of street connected to property. \n    # No 0 so nan is likely to mean \"property not connected to the street\"\n    missing_val_replace[\"LotFrontage\"] = 0\n\n    # Type of alley access: nan, 'Grvl' or 'Pave'\n    # nan probably means \"no alley\", replace with 'None'\n    missing_val_replace[\"Alley\"] = \"None\"\n    \n    # Exterior covering material. Set to mode.\n    missing_val_replace[\"Exterior1st\"] = data[\"Exterior1st\"].mode()[0]\n    missing_val_replace[\"Exterior2nd\"] = data[\"Exterior2nd\"].mode()[0]\n\n    # Masonry veneer type and area in square feet\n    # 'None' is already a value and 0 exists as an area\n    missing_val_replace[\"MasVnrType\"] = \"None\"\n    missing_val_replace[\"MasVnrArea\"] = 0\n\n    # When BsmtFinType1 is missing, BsmtFinSF1 equals 0. Also true when \n    # BsmtFinType1 is \"Unf\"(inished) which makes sense.\n    missing_val_replace[\"BsmtFinType1\"] = \"Unf\"\n    # same as BsmtFinType1 except for one case where BsmtFinSF2 equals\n    # 479. Still keeping 'Unf' as an approximation.\n    missing_val_replace[\"BsmtFinType2\"] = \"Unf\"\n\n    # Same as above for BsmtQual and value 'Ta'\n    missing_val_replace[\"BsmtQual\"] = \"TA\"\n    # same as BsmtQual\n    missing_val_replace[\"BsmtCond\"] = \"TA\"\n\n    # Not as straightforward as above set missing values as \n    # the mode of the column\n    missing_val_replace[\"BsmtExposure\"] = data[\"BsmtExposure\"].mode()[0]\n    \n    # Basement (un)finished/total square feet. Set to 0 because there is no basement\n    missing_val_replace[\"BsmtFinSF1\"] = 0\n    missing_val_replace[\"BsmtFinSF2\"] = 0\n    missing_val_replace[\"BsmtUnfSF\"] = 0\n    missing_val_replace[\"TotalBsmtSF\"] = 0\n    \n    # Basement bathrooms. Set to 0 because there is no basement\n    missing_val_replace[\"BsmtFullBath\"] = 0\n    missing_val_replace[\"BsmtHalfBath\"] = 0\n    \n    # only one missing value, set mode\n    missing_val_replace[\"Electrical\"] = data[\"Electrical\"].mode()[0]\n\n    # Kitchen quality, since in the only missing case, the number of \n    # kitchen (KitchenAbvGr) is 1, we set it to the mode\n    missing_val_replace[\"KitchenQual\"] = data[\"KitchenQual\"].mode()[0]\n    \n    # Home functionality rating; cannot be guessed. Set the mode.\n    missing_val_replace[\"Functional\"] = data[\"Functional\"].mode()[0]\n    \n    # missing if there is no fireplace (Fireplaces equals 0). set to \"None\"\n    missing_val_replace[\"FireplaceQu\"] = \"None\"\n\n    # For the test set, when one of GarageType, GarageYrBlt, GarageFinish, GarageQual, \n    # GarageCond is missing, all the others are missing and GarageCars, GarageArea equal 0. \n    # Thus there is no garage, we set \"None\" for categorical variable and to the median for \n    # GarageYrBlt to avoid type issues.\n    # For the test set, GarageType is set (so there is one) but the info is missing.\n    # Since only one entry has this problem, we keep the same idea as for the train set\n    # even though using the mode instead of \"None\" could be better.\n    missing_val_replace[\"GarageType\"] = \"None\"\n    missing_val_replace[\"GarageYrBlt\"]= np.round(data[\"GarageYrBlt\"].median())\n    missing_val_replace[\"GarageCars\"]= np.round(data[\"GarageCars\"].median())\n    missing_val_replace[\"GarageArea\"]= np.round(data[\"GarageArea\"].median())\n    missing_val_replace[\"GarageFinish\"]= \"None\"\n    missing_val_replace[\"GarageQual\"]= \"None\"\n    missing_val_replace[\"GarageCond\"] = \"None\"\n\n    # When PoolQc is missing, PoolArea is 0 because there is no pool. Set to \"None\".\n    missing_val_replace[\"PoolQC\"] = \"None\"\n\n    # No fence, set to \"None\"\n    missing_val_replace[\"Fence\"] = \"None\"\n\n    # Probably no special features, set to \"None\"\n    missing_val_replace[\"MiscFeature\"] = \"None\"\n\n    missing_val_replace[\"SaleType\"] = data[\"SaleType\"].mode()[0]\n    return missing_val_replace\n\n# Add a \"was missing\" features in case the missing values\n# were not random\nif add_was_missing_features:\n    for col in feature_df:\n        if np.any(pd.isnull(feature_df[col])) or np.any(pd.isnull(test_df[col])):\n            feature_df[\"Missing_\" + col] = pd.isnull(feature_df[col])\n            test_df[\"Missing_\" + col] = pd.isnull(test_df[col])\n\n# replace missing values\n# replacement_dict = get_replacement(feature_df, test_df)\n# feature_df.fillna(replacement_dict, inplace=True)\n# test_df.fillna(replacement_dict, inplace=True)    \nfeature_df.fillna(get_replacement(feature_df), inplace=True)\ntest_df.fillna(get_replacement(test_df), inplace=True)    \n\n# sanity check\nprint(\"Remaining missing values in train and test sets:\")\nprint(np.sum((feature_df.shape[0] - feature_df.count()) != 0))\nprint(np.sum((test_df.shape[0] - test_df.count()) != 0))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f5646b01-7d7b-4294-b4b9-63d2e3582c28",
        "collapsed": true,
        "_uuid": "a26c4b1b6835024a585b91c2164221175cb6b6a9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Getting the right column types\n# reference: https://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt\n\ndef ordinal_object_to_str(df):\n    df[\"LotShape\"] = df[\"LotShape\"].astype(str)\n    df[\"Utilities\"] = df[\"Utilities\"].astype(str)\n    df[\"LandSlope\"] = df[\"LandSlope\"].astype(str)\n    df[\"ExterQual\"] = df[\"ExterQual\"].astype(str)\n    df[\"ExterCond\"] = df[\"ExterCond\"].astype(str)\n    df[\"BsmtQual\"] = df[\"BsmtQual\"].astype(str)\n    df[\"BsmtCond\"] = df[\"BsmtCond\"].astype(str)\n    df[\"BsmtExposure\"] = df[\"BsmtExposure\"].astype(str)\n    df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].astype(str)\n    df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].astype(str)\n    df[\"HeatingQC\"] = df[\"HeatingQC\"].astype(str)\n    df[\"Electrical\"] = df[\"Electrical\"].astype(str)\n    df[\"KitchenQual\"] = df[\"KitchenQual\"].astype(str)\n    df[\"Functional\"] = df[\"Functional\"].astype(str)\n    df[\"FireplaceQu\"] = df[\"FireplaceQu\"].astype(str)\n    df[\"GarageQual\"] = df[\"GarageQual\"].astype(str)\n    df[\"GarageCond\"] = df[\"GarageCond\"].astype(str)\n    df[\"PavedDrive\"] = df[\"PavedDrive\"].astype(str)\n    df[\"PoolQC\"] = df[\"PoolQC\"].astype(str)\n    df[\"Fence\"] = df[\"Fence\"].astype(str)\n    return df\n\ndef fix_dtypes(df):\n    df[\"MSSubClass\"] = df[\"MSSubClass\"].astype(object)\n    \n    df[\"LotShape\"] = df[\"LotShape\"].astype(int)\n    df[\"Utilities\"] = df[\"Utilities\"].astype(int)\n    df[\"LandSlope\"] = df[\"LandSlope\"].astype(int)\n    df[\"ExterQual\"] = df[\"ExterQual\"].astype(int)\n    df[\"ExterCond\"] = df[\"ExterCond\"].astype(int)\n    df[\"BsmtQual\"] = df[\"BsmtQual\"].astype(int)\n    df[\"BsmtCond\"] = df[\"BsmtCond\"].astype(int)\n    df[\"BsmtExposure\"] = df[\"BsmtExposure\"].astype(int)\n    df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].astype(int)\n    df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].astype(int)\n    df[\"HeatingQC\"] = df[\"HeatingQC\"].astype(int)\n    df[\"Electrical\"] = df[\"Electrical\"].astype(int)\n    df[\"KitchenQual\"] = df[\"KitchenQual\"].astype(int)\n    df[\"Functional\"] = df[\"Functional\"].astype(int)\n    df[\"FireplaceQu\"] = df[\"FireplaceQu\"].astype(int)\n    df[\"GarageQual\"] = df[\"GarageQual\"].astype(int)\n    df[\"GarageCond\"] = df[\"GarageCond\"].astype(int)\n    df[\"PavedDrive\"] = df[\"PavedDrive\"].astype(int)\n    df[\"PoolQC\"] = df[\"PoolQC\"].astype(int)\n    df[\"Fence\"] = df[\"Fence\"].astype(int)\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].astype(int)\n\n    df[\"LotArea\"] = df[\"LotArea\"].astype(float)\n    df[\"BsmtFinSF1\"] = df[\"BsmtFinSF1\"].astype(float)\n    df[\"BsmtFinSF2\"] = df[\"BsmtFinSF2\"].astype(float)\n    df[\"BsmtUnfSF\"] = df[\"BsmtUnfSF\"].astype(float)\n    df[\"TotalBsmtSF\"] = df[\"TotalBsmtSF\"].astype(float)\n    df[\"1stFlrSF\"] = df[\"1stFlrSF\"].astype(float)\n    df[\"2ndFlrSF\"] = df[\"2ndFlrSF\"].astype(float)\n    df[\"LowQualFinSF\"] = df[\"LowQualFinSF\"].astype(float)\n    df[\"GrLivArea\"] = df[\"GrLivArea\"].astype(float)\n    df[\"GarageArea\"] = df[\"GarageArea\"].astype(float)\n    df[\"WoodDeckSF\"] = df[\"WoodDeckSF\"].astype(float)\n    df[\"OpenPorchSF\"] = df[\"OpenPorchSF\"].astype(float)\n    df[\"EnclosedPorch\"] = df[\"EnclosedPorch\"].astype(float)\n    df[\"3SsnPorch\"] = df[\"3SsnPorch\"].astype(float)\n    df[\"ScreenPorch\"] = df[\"ScreenPorch\"].astype(float)\n    df[\"PoolArea\"] = df[\"PoolArea\"].astype(float)\n    df[\"MiscVal\"] = df[\"MiscVal\"].astype(float)\n    \n    return df\n\nordinal_replacements = {}\nordinal_replacements[\"LotShape\"] = {\"Reg\": \"0\", \"IR1\": \"1\", \"IR2\": \"2\", \"IR3\": \"3\"}\nordinal_replacements[\"Utilities\"] = {\"AllPub\": \"0\", \"NoSewr\": \"1\", \"NoSeWa\": \"2\", \"ELO\": \"3\"}\nordinal_replacements[\"LandSlope\"] = {\"Gtl\": \"0\", \"Mod\": \"1\", \"Sev\": \"2\"}\nordinal_replacements[\"ExterQual\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"ExterCond\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"BsmtQual\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"BsmtCond\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"BsmtExposure\"] = {\"Gd\": \"0\", \"Av\": \"1\", \"Mn\": \"2\", \"No\": \"3\"}\nordinal_replacements[\"BsmtFinType1\"] = {\"GLQ\": \"0\", \"ALQ\": \"1\", \"BLQ\": \"2\", \"Rec\": \"3\", \"LwQ\": \"4\", \"Unf\": \"5\"}\nordinal_replacements[\"BsmtFinType2\"] = {\"GLQ\": \"0\", \"ALQ\": \"1\", \"BLQ\": \"2\", \"Rec\": \"3\", \"LwQ\": \"4\", \"Unf\": \"5\"}\nordinal_replacements[\"HeatingQC\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"Electrical\"] = {\"SBrkr\": \"0\", \"FuseA\": \"1\", \"FuseF\": \"2\", \"FuseP\": \"3\", \"Mix\": \"4\"}\nordinal_replacements[\"KitchenQual\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\"}\nordinal_replacements[\"Functional\"] = {\"Typ\": \"0\", \"Min1\": \"1\", \"Min2\": \"2\", \"Mod\": \"3\", \"Maj1\": \"4\", \n                                      \"Maj2\": \"5\", \"Sev\": \"6\", \"Sal\": \"7\"}\nordinal_replacements[\"FireplaceQu\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\", \"None\": 5}\nordinal_replacements[\"GarageQual\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\", \"None\": 5}\nordinal_replacements[\"GarageCond\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\", \"None\": 5}\nordinal_replacements[\"PavedDrive\"] = {\"Y\": \"0\", \"P\": \"1\", \"N\": \"2\"}\nordinal_replacements[\"PoolQC\"] = {\"Ex\": \"0\", \"Gd\": \"1\", \"TA\": \"2\", \"Fa\": \"3\", \"Po\": \"4\", \"None\": 5}\nordinal_replacements[\"Fence\"] = {\"GdPrv\": \"0\", \"MnPrv\": \"1\", \"GdWo\": \"2\", \"MnWw\": \"3\", \"None\": 5}\n\nfeature_df = ordinal_object_to_str(feature_df)\nfeature_df.replace(ordinal_replacements, inplace=True)\nfeature_df = fix_dtypes(feature_df)\n\ntest_df = ordinal_object_to_str(test_df)\ntest_df.replace(ordinal_replacements, inplace=True)\ntest_df = fix_dtypes(test_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e54625b7-032a-456d-9a0f-d5dc15fdb235",
        "collapsed": true,
        "_uuid": "6f84e5033b961f22f872bd10669e003bc79b5fb1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# unskewing can be usefull for regression\n# Props to https://www.kaggle.com/apapiu/regularized-linear-models\ndef unskew_dataset(data):\n    numeric_features = data.dtypes[data.dtypes == float].index\n    skewed_features = data[numeric_features].apply(lambda x: sp.stats.skew(x)) #compute skewness\n    skewed_features = skewed_features[skewed_features > 0.75]\n    skewed_features = skewed_features.index\n    data[skewed_features] = np.log1p(data[skewed_features])\n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a78b5667-8c88-4d04-a2a7-7c0bfff71462",
        "_uuid": "79c6aa9ef9ee27abd3fae0b53c3d1cd1517eace5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Total surface.\n# Propos to https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\nfeature_df['TotalSF'] = feature_df['TotalBsmtSF'] + feature_df['1stFlrSF'] + feature_df['2ndFlrSF']\ntest_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n\n# encode categorical variables as numeric values for sklearn\ncategorical_vars_indices = np.where((feature_df.dtypes == object))[0]\ncategorical_vars = feature_df.columns[categorical_vars_indices]\n\n# ft_df = pd.concat([feature_df, test_df])\n# unskew features\n# ft_df = unskew_dataset(ft_df)\nfeature_df = unskew_dataset(feature_df)\ntest_df = unskew_dataset(test_df)\n# encode categorical variables as dummies (one hot) for linear regressors\n# ft_df_dummies = pd.get_dummies(ft_df, columns=categorical_vars, \n#                                     drop_first=True, sparse=False)\n# feature_df_dummies = ft_df_dummies.iloc[range(feature_df.shape[0]), :]\n# test_df_dummies = ft_df_dummies.iloc[range(feature_df.shape[0], ft_df_dummies.shape[0]), :]\nfeature_df_dummies = pd.get_dummies(feature_df, columns=categorical_vars, \n                                    drop_first=True, sparse=False)\ntest_df_dummies = pd.get_dummies(test_df, columns=categorical_vars, \n                                 drop_first=True, sparse=False)\n\n# Only keep columns common to both the train and test sets\n# so that they have the same features\ncommon_cols = list(set(feature_df_dummies.columns) & set(test_df_dummies.columns))\nfeature_df_dummies = feature_df_dummies[common_cols]\ntest_df_dummies = test_df_dummies[common_cols]\n\nlabel_enc = LabelEncoder()\n# fit on the concatenated train and test sets to get the same encoding \n# for them both\nfor var in categorical_vars:\n    var_all = pd.concat([feature_df.loc[:, var], test_df.loc[:, var]])\n    label_enc.fit(var_all)\n    feature_df.loc[:, var] = label_enc.transform(feature_df.loc[:, var])\n    test_df.loc[:, var] = label_enc.transform(test_df.loc[:, var])\n\n# sanity checks\nassert np.all(feature_df_dummies[\"LotShape\"] == feature_df[\"LotShape\"])\nassert np.all(test_df_dummies[\"LotShape\"] == test_df[\"LotShape\"])\nprint(feature_df.shape)\nprint(feature_df_dummies.shape)\nprint(test_df.shape)\nprint(test_df_dummies.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e911eaa0-8c8f-4427-8faa-2b538b41bdf8",
        "collapsed": true,
        "_uuid": "fe774ac93ab1e7effe48eb485a68de15b3ee0ab3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# standardize the variables.\nscaler = StandardScaler()\n\nfeature_df[:] = scaler.fit_transform(feature_df)\nfeature_df_dummies[:] = scaler.fit_transform(feature_df_dummies)\ntest_df[:] = scaler.fit_transform(test_df)\ntest_df_dummies[:] = scaler.fit_transform(test_df_dummies)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0181fb9e-40b0-4db8-a1b8-0a54d9b70804",
        "collapsed": true,
        "_uuid": "ed34ed51f474d3a920b8994665ad54b0dc66e06f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "rkf_cv = KFold(n_splits=5, random_state=RNG_SEED)\nstack_folds = list(KFold(n_splits=5, random_state=RNG_SEED).split(feature_df))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8dc98e57-7c21-4e92-97d6-1875c7b4dc34",
        "_uuid": "cfb2eb46b865e668a9d30b5463a2bb3a2e5cdec4",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# L1 + L2 penalized linear regression\n\nl1_ratios = [.1, .5, .7, .9, .95, .99, 1]\nalphas = alphas=[1] + [10 ** -x for x in range(1, 8)] + [5 * 10 ** -x for x in range(1, 8)]\n\nif not os.path.isfile(\"cv_opt_en.pkl\") or overwrite_models:\n    en_cv = ElasticNetCV(l1_ratio=l1_ratios, alphas=alphas,\n                         normalize=True, selection =\"random\", random_state=RNG_SEED,\n                         max_iter=10000, cv=RepeatedKFold(10, 3, random_state=RNG_SEED))\n    cv_opt_en = en_cv.fit(feature_df_dummies, outcome_df)\n    joblib.dump(cv_opt_en, \"cv_opt_en.pkl\")\nelse:\n    cv_opt_en = joblib.load(\"cv_opt_en.pkl\")\n\n# cross-validated rmse for the best parameters\nl1_ratio_index = np.where(l1_ratios == cv_opt_en.l1_ratio_)[0][0]\nen_alpha_index = np.where(cv_opt_en.alphas_ == cv_opt_en.alpha_)[0][0]\nen_rmse = np.sqrt(np.mean(cv_opt_en.mse_path_, axis=2)[l1_ratio_index, en_alpha_index])\nprint(en_rmse)\nprint(cv_opt_en)\n    \n# model using the best parameters so that the cross-validation \n# does not run for every fold when we cross_val_predict() \n# (to reduce the computation time)\ncv_opt_en_model = ElasticNet(alpha=cv_opt_en.alpha_, l1_ratio=cv_opt_en.l1_ratio_, \n                         fit_intercept=True, normalize=True, \n                         precompute=False, max_iter=10000, copy_X=True, tol=0.0001, \n                         warm_start=False, positive=False, random_state=RNG_SEED, \n                         selection=\"random\")\ncv_opt_en_model = cv_opt_en_model.fit(feature_df_dummies, outcome_df)\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_en_model = ElasticNet(alpha=0.0003, l1_ratio=0.3, fit_intercept=True, normalize=True, \n                   precompute=False, max_iter=10000, copy_X=True, tol=0.0001, \n                   warm_start=False, positive=False, random_state=RNG_SEED, \n                   selection=\"random\")\ncv_opt_en_model = cv_opt_en_model.fit(feature_df_dummies, outcome_df)\nen_rmse = 0.136357599601\n\"\"\"\n\nen_preds = cv_opt_en_model.predict(test_df_dummies)\nen_cv_preds = cross_val_predict(cv_opt_en_model, feature_df_dummies, outcome_df, \n                                cv=stack_folds)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nfor i in range(cv_opt_en.mse_path_.shape[0]):\n    ax.plot(np.log10(cv_opt_en.alphas_), np.mean(cv_opt_en.mse_path_[i, :, :], axis=1),\n             label=l1_ratios[i])\nax.set_title((\"Elastic net regularization path (L1 / alpha vs rmse)\\n\"\n             \"best params: %s, %s\" % (cv_opt_en.l1_ratio_, cv_opt_en.alpha_)))\nplt.legend()\n\nfig = plt.figure(figsize=(8, 50))\nax = fig.add_subplot(111)\nax.barh(np.arange(len(cv_opt_en.coef_), 0, -1), cv_opt_en.coef_,\n       tick_label=feature_df_dummies.columns,)\nax.set_title(\"Elastic network coefs\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e2cc2b94-b754-4508-bf3a-57e0f84745bf",
        "collapsed": true,
        "_uuid": "164f78bc6df315a9bc698e1fe906b776ef96b52d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# L1 penalized linear regression\n\nalphas = [1] + [10 ** -x for x in range(1, 8)] + [5 * 10 ** -x for x in range(1, 8)]\nif not os.path.isfile(\"cv_opt_ll.pkl\") or overwrite_models:\n    ll_cv = LassoCV(alphas=alphas,\n                    max_iter=10000, normalize=False, \n                    cv=RepeatedKFold(10, 3, random_state=RNG_SEED),\n                    random_state=RNG_SEED, selection=\"random\")\n    cv_opt_ll = ll_cv.fit(feature_df_dummies, outcome_df)\n    joblib.dump(cv_opt_ll, \"cv_opt_ll.pkl\")\nelse: \n    cv_opt_ll = joblib.load(\"cv_opt_ll.pkl\")\n    \n# cross-validated rmse for the best parameters\nll_alpha_index = np.where(cv_opt_ll.alphas_ == cv_opt_ll.alpha_)[0][0]\nll_rmse = np.sqrt(np.mean(cv_opt_ll.mse_path_, axis=1)[ll_alpha_index])\nprint(ll_rmse)\nprint(cv_opt_ll)\n\n# model using the best parameters so that the cross-validation \n# does not run for every fold when we cross_val_predict() \n# (to reduce the computation time)\ncv_opt_ll_model = Lasso(alpha=cv_opt_ll.alpha_, fit_intercept=True, normalize=False, \n                        precompute=False, copy_X=True, max_iter=10000, \n                        tol=0.0001, warm_start=False, positive=False, \n                        random_state=RNG_SEED, selection=\"random\")\ncv_opt_ll_model = cv_opt_ll_model.fit(feature_df_dummies, outcome_df)\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_ll_model = Lasso(alpha=0.0001, fit_intercept=True, verbose=False,\n                  precompute=\"auto\", max_iter=10000, \n                  eps=2.2204460492503131e-16, copy_X=True, \n                  fit_path=True, positive=False)\ncv_opt_ll_model = cv_opt_ll_model.fit(feature_df_dummies, outcome_df)\nll_rmse = 0.14276387188\n\"\"\"\n\nll_preds = cv_opt_ll_model.predict(test_df_dummies)\nll_cv_preds = cross_val_predict(cv_opt_ll_model, feature_df_dummies, outcome_df, \n                                cv=stack_folds)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(np.log10(cv_opt_ll.alphas_), np.mean(cv_opt_ll.mse_path_, axis=1))\nax.set_title((\"lars lasso regularization path (alpha vs rmse)\\n\"\n             \"best alpha = %s\" % cv_opt_ll.alpha_))\nplt.legend()\n\nfig = plt.figure(figsize=(8, 50))\nax = fig.add_subplot(111)\nax.barh(np.arange(len(cv_opt_ll.coef_), 0, -1), cv_opt_ll.coef_,\n       tick_label=feature_df_dummies.columns,)\nax.set_title(\"Lars lasso coefs\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a38bf5c0-1d43-43d5-9e2a-1d0a959690d6",
        "collapsed": true,
        "_uuid": "4e686528fc3aa294c7ae0e43a47c669fda377ee7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# k nearest neighbors regression\n# l_1, l_2, l_p, l_inf\nmetrics = [\"euclidean\", \"manhattan\", \"minkowski\", \"chebyshev\"]\nn_neighbors_list = np.arange(4, 11, 1)\n\nif not os.path.isfile(\"cv_opt_kn.pkl\") or overwrite_models:\n    kn = KNeighborsRegressor(n_jobs=4, p=3)\n    kn_param_grid = {\"n_neighbors\": n_neighbors_list,\n                    \"weights\": [\"uniform\", \"distance\"],\n                    \"metric\": metrics}\n    kn_gs = GridSearchCV(estimator=kn, param_grid=kn_param_grid, scoring=\"neg_mean_squared_error\", \n                         fit_params=None, cv=rkf_cv)\n    cv_opt_kn = kn_gs.fit(feature_df, outcome_df)\n    joblib.dump(cv_opt_kn, \"cv_opt_kn.pkl\")\nelse:\n    cv_opt_kn = joblib.load(\"cv_opt_kn.pkl\")\n\n# NB: loss is negative mean squared error\nkn_rmse = np.sqrt(-cv_opt_kn.best_score_)\nprint(cv_opt_kn.best_score_, kn_rmse)\nprint(cv_opt_kn.best_estimator_)\n    \ncv_opt_kn_model = cv_opt_kn.best_estimator_\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_kn_model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n            metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n            weights='distance')\ncv_opt_kn_model = cv_opt_kn_model.fit(feature_df, outcome_df) \nkn_rmse = 0.198152549818\n\"\"\"\n\nkn_preds = cv_opt_kn_model.predict(test_df)\nkn_cv_preds = cross_val_predict(cv_opt_kn_model, feature_df, outcome_df, cv=stack_folds)\n\nuniform_run = cv_opt_kn.cv_results_[\"param_weights\"] == \"uniform\"\ndistance_run = cv_opt_kn.cv_results_[\"param_weights\"] == \"distance\"\nbest_metric = cv_opt_kn.best_params_[\"metric\"]\nhas_best_metric = cv_opt_kn.cv_results_[\"param_metric\"] == best_metric\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(n_neighbors_list, \n        np.sqrt(-cv_opt_kn.cv_results_[\"mean_test_score\"][uniform_run & has_best_metric]),\n       label=\"uniform\")\nax.plot(n_neighbors_list, \n        np.sqrt(-cv_opt_kn.cv_results_[\"mean_test_score\"][distance_run & has_best_metric]),\n       label=\"distance\")\nax.set_title(\"Knn CV (%s) (#nn / weights vs rmse)\\nBest params: %s, %s\" % \\\n             tuple(list(cv_opt_kn.best_params_.values())))\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "67edc6df-8ba4-4dc1-bc3f-8f341dc1dc24",
        "collapsed": true,
        "_uuid": "44483a1a34c65f9afe4c59ab8eb39a6f7bf1fc9d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Gradient boosted trees regression\n\nif not os.path.isfile(\"cv_opt_xgb.pkl\") or overwrite_models:\n    xgb = XGBRegressor(random_state=RNG_SEED, n_estimators=500, n_jobs=4)\n    reg_ratios = [0.1, 0.5, 0.9]\n    xgb_param_grid = {\"max_depth\": [1, 2, 3, 5],\n                      \"learning_rate\": [0.05, 0.1, 0.2],\n                      \"reg_lambda\": reg_ratios,\n                      \"reg_alpha\": reg_ratios}\n    xgb_gs = GridSearchCV(estimator=xgb, param_grid=xgb_param_grid, \n                          scoring=\"neg_mean_squared_error\", \n                          fit_params=None, cv=rkf_cv)\n    cv_opt_xgb = xgb_gs.fit(feature_df, outcome_df)\n    joblib.dump(cv_opt_xgb, \"cv_opt_xgb.pkl\")\nelse:\n    cv_opt_xgb = joblib.load(\"cv_opt_xgb.pkl\")\n    \n# NB: loss is negative mean squared error\nxgb_rmse = np.sqrt(-cv_opt_xgb.best_score_)\nprint(cv_opt_xgb.best_score_, xgb_rmse)\nprint(cv_opt_xgb.best_estimator_)\n\ncv_opt_xgb_model = cv_opt_xgb.best_estimator_\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_xgb_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=2, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=None, objective='reg:linear',\n       random_state=RNG_SEED, reg_alpha=0.1, reg_lambda=0.5,\n       scale_pos_weight=1, seed=None, silent=True, subsample=1)\ncv_opt_xgb_model =  cv_opt_xgb_model.fit(feature_df, outcome_df)\nxgb_rmse = 0.123629669672\n\"\"\"\n\nxgb_preds = cv_opt_xgb_model.predict(test_df)\nxgb_cv_preds = cross_val_predict(cv_opt_xgb_model, feature_df, outcome_df, cv=stack_folds)\n\n# feature importances\nfig = plt.figure(figsize=(8, 30))\nax = fig.add_subplot(111)\nax.barh(np.arange(len(cv_opt_xgb_model.feature_importances_), 0, -1), \n        cv_opt_xgb_model.feature_importances_,\n        tick_label=feature_df.columns)\nax.set_title(cv_opt_xgb.best_score_)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "856cd854-d3cf-4774-87c3-638f36f05ec4",
        "collapsed": true,
        "_uuid": "b24dc6dda67e1fb103be65394427ff3619aeb4ec",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Gradient boosted trees regression (again)\n\nif not os.path.isfile(\"cv_opt_lgb.pkl\") or overwrite_models:\n    lgb = LGBMRegressor(random_state=RNG_SEED, n_estimators=500, n_jobs=4)\n    reg_ratios = [0.1, 0.5, 0.9]\n    lgb_param_grid = {\"max_depth\": [1, 3, 5, -1],\n                      \"learning_rate\": [0.05, 0.1, 0.2],\n                      \"reg_lambda\": reg_ratios,\n                      \"reg_alpha\": reg_ratios}\n    lgb_gs = GridSearchCV(estimator=lgb, param_grid=lgb_param_grid, \n                          scoring=\"neg_mean_squared_error\", \n                          fit_params=None, cv=rkf_cv)\n    cv_opt_lgb = lgb_gs.fit(feature_df, outcome_df)\n    joblib.dump(cv_opt_lgb, \"cv_opt_lgb.pkl\")\nelse:\n    cv_opt_lgb = joblib.load(\"cv_opt_lgb.pkl\")\n    \n# NB: loss is negative mean squared error\nlgb_rmse = np.sqrt(-cv_opt_lgb.best_score_)\nprint(cv_opt_lgb.best_score_, lgb_rmse)\nprint(cv_opt_lgb.best_estimator_)\n\ncv_opt_lgb_model = cv_opt_lgb.best_estimator_\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_lgb_model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n       learning_rate=0.05, max_depth=3, min_child_samples=20,\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n       n_jobs=-1, num_leaves=31, objective=None, random_state=RNG_SEED,\n       reg_alpha=0.5, reg_lambda=0.5, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=1)\ncv_opt_lgb_model =  cv_opt_lgb_model.fit(feature_df, outcome_df)\nlgb_rmse = 0.128340967645\n\"\"\"\nlgb_preds = cv_opt_lgb_model.predict(test_df)\nlgb_cv_preds = cross_val_predict(cv_opt_lgb_model, feature_df, outcome_df, cv=stack_folds)\n\n# feature importances\nfig = plt.figure(figsize=(8, 30))\nax = fig.add_subplot(111)\nax.barh(np.arange(len(cv_opt_lgb_model.feature_importances_), 0, -1), \n        cv_opt_lgb_model.feature_importances_,\n        tick_label=feature_df.columns)\nax.set_title(cv_opt_lgb.best_score_)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f73e6ce5-8e09-4a46-b59c-18e8a5d9327c",
        "collapsed": true,
        "_uuid": "5f93b3f82e9744095ec982dfc6cf5419703da2ce",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# random forest\n\nif not os.path.isfile(\"cv_opt_rf.pkl\") or overwrite_models:\n    rf = RandomForestRegressor(n_estimators=500, random_state=RNG_SEED, n_jobs=4)\n    rf_param_grid = {\"min_samples_split\": [1.0, 3, 5],\n                      \"max_features\": [\"sqrt\", \"log2\"]}\n    rf_gs = GridSearchCV(estimator=rf, param_grid=rf_param_grid, scoring=\"neg_mean_squared_error\", \n                         fit_params=None, cv=rkf_cv)\n    cv_opt_rf = rf_gs.fit(feature_df, outcome_df)\n    joblib.dump(cv_opt_rf, \"cv_opt_rf.pkl\")\nelse:\n    cv_opt_rf = joblib.load(\"cv_opt_rf.pkl\")\n    \n# NB: loss is negative mean squared error\nrf_rmse = np.sqrt(-cv_opt_rf.best_score_)\nprint(cv_opt_rf.best_score_, rf_rmse)\nprint(cv_opt_rf.best_estimator_)\n\ncv_opt_rf_model = cv_opt_rf.best_estimator_\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_rf_model = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=3,\n           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n           oob_score=False, random_state=RNG_SEED, verbose=0,\n           warm_start=False)\ncv_opt_rf_model = cv_opt_rf_model.fit(feature_df, outcome_df)\nrf_rmse = 0.141737550372\n\"\"\"\n\nrf_preds = cv_opt_rf_model.predict(test_df)\nrf_cv_preds = cross_val_predict(cv_opt_rf_model, feature_df, outcome_df, cv=stack_folds)\n\n# feature importances\nfig = plt.figure(figsize=(8, 30))\nax = fig.add_subplot(111)\nax.barh(np.arange(len(cv_opt_rf_model.feature_importances_), 0, -1), \n        cv_opt_rf_model.feature_importances_,\n        tick_label=feature_df.columns)\nax.set_title(cv_opt_rf.best_score_)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "19cf4d2a-6d44-45e1-ba53-dc1c5f77dcb6",
        "collapsed": true,
        "_uuid": "f1d00ea576b519f39b73a9ff153d858531e87a3c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# gaussian process\n\n# Note: the kernel’s hyperparameters are optimized during fitting\nif not os.path.isfile(\"cv_opt_gp.pkl\") or overwrite_models:\n    gp = GaussianProcessRegressor(normalize_y=True, random_state=RNG_SEED)\n    gp_param_grid = {\"kernel\": [RBF() + WhiteKernel(), RationalQuadratic() + WhiteKernel()]}\n    gp_gs = GridSearchCV(estimator=gp, param_grid=gp_param_grid, scoring=\"neg_mean_squared_error\", \n                         fit_params=None, cv=rkf_cv)\n    cv_opt_gp = gp_gs.fit(feature_df, outcome_df)\n    joblib.dump(cv_opt_gp, \"cv_opt_gp.pkl\")  \nelse:\n    cv_opt_gp = joblib.load(\"cv_opt_gp.pkl\")\n\n# NB: loss is negative mean squared error    \ngp_rmse = np.sqrt(-cv_opt_gp.best_score_)\nprint(cv_opt_gp.best_score_, gp_rmse)\nprint(cv_opt_gp.best_estimator_)\n\ncv_opt_gp_model = cv_opt_gp.best_estimator_\n\"\"\"\n# From a previous run of what is above, for the submission\ncv_opt_gp_model = GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n             kernel=RationalQuadratic(alpha=1, length_scale=1) + WhiteKernel(noise_level=1),\n             n_restarts_optimizer=0, normalize_y=True,\n             optimizer='fmin_l_bfgs_b', random_state=RNG_SEED)\ncv_opt_gp_model = cv_opt_gp_model.fit(feature_df, outcome_df)\ngp_rmse = 0.132643420585\n\"\"\"\n\ngp_preds = cv_opt_gp_model.predict(test_df)\ngp_cv_preds = cross_val_predict(cv_opt_gp_model, feature_df, outcome_df, cv=stack_folds)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4e1efcfe-5969-4027-9a34-7d77bab9b605",
        "collapsed": true,
        "_uuid": "5d18540d44be44ac1da16d132090ec689f9bd487",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# model stacking\n\n# fold predictions of the base learners to be used for training the\n# meta model\nbase_stack_cv_preds = np.vstack([en_cv_preds, ll_cv_preds, kn_cv_preds,\n                                 xgb_cv_preds, lgb_cv_preds, rf_cv_preds, \n                                 gp_cv_preds]).T\n\n# meta model\n# meta = LinearRegression()\nmeta = KernelRidge()\n# cross-validation (note: using the same stack folds) to assess \n# the accuracy of the meta model\nstack_cv_preds = cross_val_predict(meta, base_stack_cv_preds, outcome_df, cv=stack_folds)\n# rmse\nprint(np.sqrt(np.mean((stack_cv_preds - outcome_df) ** 2)))\n\n# train the  meta model on the fold predictions from the base models\nmeta = meta.fit(base_stack_cv_preds, outcome_df)\n# print(meta.coef_)\nbase_stack_test_preds = np.vstack([en_preds, ll_preds, kn_preds,\n                                   xgb_preds, lgb_preds, rf_preds, \n                                   gp_preds]).T\n\n# use the base models' full predictions as input to the meta model\ntest_preds = meta.predict(base_stack_test_preds)\nfinal_preds = np.exp(test_preds)      \n\n\"\"\"\n# basic weighting scheme\nerrors = np.array([en_rmse, ll_rmse, kn_rmse, xgb_rmse, rf_rmse, gp_rmse])\nprint(errors)\nmodel_weights = np.exp(1 / np.array(errors))\n# model_weights = 1 / np.array(errors)\nmodel_weights /= np.sum(model_weights)\nprint(model_weights)\n\ndon't forget to turn the log price into price, silly you!\nfinal_preds = np.exp(np.sum(all_preds * model_weights, axis=1))\n\"\"\"      \n\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": final_preds})\nsubmission.to_csv('house_prices_submission.csv', index=False)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dee74572-9583-49ee-b205-c8111ac0af94",
        "collapsed": true,
        "_uuid": "367fa13932ec65264b3517985ab99a66cf4d415b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "all_preds = np.array([en_preds, ll_preds, kn_preds, xgb_preds, \n                      lgb_preds, rf_preds, gp_preds]).T\nplt.figure()\nsns.pairplot(pd.DataFrame(all_preds))\nplt.figure()\nsns.heatmap(np.corrcoef(all_preds.T))\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}